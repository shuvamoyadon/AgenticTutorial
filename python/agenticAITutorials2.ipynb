{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Single Python script that integrates OpenAI GPT-4, Anthropic Claude, and Google Gemini into one app. It uses a router function to choose the model dynamically.\n",
    "# multi_llm_router.py\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Set up API keys ---\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=google_key)\n",
    "\n",
    "# --- GPT-4 ---\n",
    "def call_gpt4(prompt: str) -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# --- Claude (Sonnet or Opus) ---\n",
    "def call_claude(prompt: str) -> str:\n",
    "    client = anthropic.Anthropic(api_key=anthropic_key)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",  # or use \"claude-3-opus-20240229\"\n",
    "        max_tokens=300,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# --- Gemini ---\n",
    "def call_gemini(prompt: str) -> str:\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# --- Unified Router ---\n",
    "def call_llm(model_name: str, prompt: str) -> str:\n",
    "    if model_name == \"gpt4\":\n",
    "        return call_gpt4(prompt)\n",
    "    elif model_name == \"claude\":\n",
    "        return call_claude(prompt)\n",
    "    elif model_name == \"gemini\":\n",
    "        return call_gemini(prompt)\n",
    "    else:\n",
    "        return \"Invalid model name. Choose from: gpt4, claude, gemini.\"\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    model = input(\"Enter model (gpt4 / claude / gemini): \").strip().lower()\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "\n",
    "    result = call_llm(model, prompt)\n",
    "    print(\"\\nüß† LLM Response:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ LLM Response Comparison and Scoring Script\n",
    "\n",
    "```python\n",
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Configure API keys ---\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=google_key)\n",
    "\n",
    "# --- GPT-4 ---\n",
    "def call_gpt4(prompt: str) -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# --- Claude ---\n",
    "def call_claude(prompt: str) -> str:\n",
    "    client = anthropic.Anthropic(api_key=anthropic_key)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# --- Gemini ---\n",
    "def call_gemini(prompt: str) -> str:\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# --- Scoring function ---\n",
    "def score_response(response: str) -> int:\n",
    "    score = 0\n",
    "    if len(response) > 100:\n",
    "        score += 1\n",
    "    if any(word in response.lower() for word in [\"however\", \"therefore\", \"in conclusion\"]):\n",
    "        score += 1\n",
    "    if \"?\" in response:\n",
    "        score += 1\n",
    "    return score\n",
    "\n",
    "# --- Compare and score all models ---\n",
    "def compare_models(prompt: str):\n",
    "    print(f\"Prompt: {prompt}\\n{'='*50}\")\n",
    "\n",
    "    models = {\n",
    "        \"GPT-4\": call_gpt4(prompt),\n",
    "        \"Claude\": call_claude(prompt),\n",
    "        \"Gemini\": call_gemini(prompt)\n",
    "    }\n",
    "\n",
    "    scores = {}\n",
    "    for name, response in models.items():\n",
    "        score = score_response(response)\n",
    "        scores[name] = {\"score\": score, \"response\": response}\n",
    "\n",
    "    # Sort and display results\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "\n",
    "    for name, result in sorted_scores:\n",
    "        print(f\"\\n### üß† {name} ‚Äî Score: {result['score']}/3\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "# --- Run comparison ---\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Explain black holes to a 10-year-old.\"\n",
    "    compare_models(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Techniques to Orchestrate Between Different AI Models\n",
    "\n",
    "Orchestration means coordinating multiple AI models to work together efficiently, based on their strengths.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Routing Based on Task Type\n",
    "\n",
    "Use different models for different types of tasks.\n",
    "\n",
    "```python\n",
    "def route_by_task(task_type, prompt):\n",
    "    if task_type == \"code\":\n",
    "        return call_gpt4(prompt)\n",
    "    elif task_type == \"summary\":\n",
    "        return call_claude(prompt)\n",
    "    elif task_type == \"vision\":\n",
    "        return call_gemini(prompt)\n",
    "\n",
    "\n",
    "## ‚úÖ 2. Fallback or Redundancy Logic\n",
    "\n",
    "Use a backup model if the primary one fails or provides a low-quality response.\n",
    "\n",
    "```python\n",
    "def call_with_fallback(prompt):\n",
    "    try:\n",
    "        return call_claude(prompt)\n",
    "    except:\n",
    "        return call_gpt4(prompt)\n",
    "\n",
    "\n",
    "## ‚úÖ 3. Model Voting or Evaluation\n",
    "\n",
    "Send the same prompt to multiple models and compare their responses using a scoring function or evaluation logic.\n",
    "\n",
    "```python\n",
    "responses = {\n",
    "    \"gpt4\": call_gpt4(prompt),\n",
    "    \"claude\": call_claude(prompt),\n",
    "    \"gemini\": call_gemini(prompt)\n",
    "}\n",
    "\n",
    "def score_response(resp):\n",
    "    score = 0\n",
    "    if \"because\" in resp.lower():\n",
    "        score += 1\n",
    "    if len(resp) > 100:\n",
    "        score += 1\n",
    "    if \"however\" in resp.lower():\n",
    "        score += 1\n",
    "    return score\n",
    "\n",
    "best_model = max(responses, key=lambda x: score_response(responses[x]))\n",
    "print(f\"‚úÖ Best model: {best_model}\")\n",
    "print(responses[best_model])\n",
    "\n",
    "\n",
    "## ‚úÖ 4. Chain of Models (Sequential Pipeline)\n",
    "\n",
    "Use the output of one model as the input to another ‚Äî useful for multi-step reasoning, document processing, or creativity pipelines.\n",
    "\n",
    "### üîó Example:\n",
    "\n",
    "```python\n",
    "# Step 1: Gemini extracts key information from an article\n",
    "facts = call_gemini(\"Extract 3 key facts from this article:\\n\" + article_text)\n",
    "\n",
    "# Step 2: Claude rewrites the facts in simpler language\n",
    "simplified = call_claude(\"Rewrite the following facts simply:\\n\" + facts)\n",
    "\n",
    "# Step 3: GPT-4 turns the simplified version into a tweet\n",
    "tweet = call_gpt4(\"Create a tweet based on these points:\\n\" + simplified)\n",
    "\n",
    "print(\"‚úÖ Final Tweet:\\n\", tweet)\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ 5. Multi-Agent Workflow\n",
    "\n",
    "Use multiple AI models as specialized agents that collaborate on a task. Each model plays a role such as **planner**, **executor**, or **reviewer** ‚Äî similar to a team working together.\n",
    "\n",
    "---\n",
    "\n",
    "### üë• Example: Blog Writing Workflow\n",
    "\n",
    "```python\n",
    "# Claude as the Planner ‚Äî creates an outline\n",
    "outline = call_claude(\"Create an outline for a blog post on 'The Future of AI in Education'.\")\n",
    "\n",
    "# GPT-4 as the Writer ‚Äî expands the outline into a full article\n",
    "blog_post = call_gpt4(\"Write a blog post based on this outline:\\n\" + outline)\n",
    "\n",
    "# Gemini as the Reviewer ‚Äî provides feedback and edits\n",
    "review = call_gemini(\"Review this blog post and suggest improvements:\\n\" + blog_post)\n",
    "\n",
    "# Display results\n",
    "print(\"üìù Blog Post:\\n\", blog_post)\n",
    "print(\"\\nüîç Reviewer Feedback:\\n\", review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_model_orchestration.py\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === API Keys (Set your keys as environment variables) ===\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=google_key)\n",
    "\n",
    "# === Model Call Functions ===\n",
    "\n",
    "def call_gpt4(prompt: str) -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def call_claude(prompt: str) -> str:\n",
    "    client = anthropic.Anthropic(api_key=anthropic_key)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=300,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text.strip()\n",
    "\n",
    "def call_gemini(prompt: str) -> str:\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "# === Step 1: Routing Based on Task Type ===\n",
    "\n",
    "def route_by_task(task_type: str, prompt: str) -> str:\n",
    "    if task_type == \"code\":\n",
    "        return call_gpt4(prompt)\n",
    "    elif task_type == \"summary\":\n",
    "        return call_claude(prompt)\n",
    "    elif task_type == \"vision\":\n",
    "        return call_gemini(prompt)\n",
    "    else:\n",
    "        return \"Invalid task type.\"\n",
    "\n",
    "# === Step 2: Fallback Logic ===\n",
    "\n",
    "def call_with_fallback(prompt: str) -> str:\n",
    "    try:\n",
    "        return call_claude(prompt)\n",
    "    except:\n",
    "        return call_gpt4(prompt)\n",
    "\n",
    "# === Step 3: Voting / Scoring ===\n",
    "\n",
    "def score_response(response: str) -> int:\n",
    "    score = 0\n",
    "    if \"because\" in response.lower():\n",
    "        score += 1\n",
    "    if len(response) > 100:\n",
    "        score += 1\n",
    "    if \"however\" in response.lower():\n",
    "        score += 1\n",
    "    return score\n",
    "\n",
    "def model_voting(prompt: str) -> str:\n",
    "    responses = {\n",
    "        \"GPT-4\": call_gpt4(prompt),\n",
    "        \"Claude\": call_claude(prompt),\n",
    "        \"Gemini\": call_gemini(prompt)\n",
    "    }\n",
    "\n",
    "    for model, resp in responses.items():\n",
    "        print(f\"\\n--- {model} ---\\n{resp}\\nScore: {score_response(resp)}\")\n",
    "\n",
    "    best_model = max(responses, key=lambda x: score_response(responses[x]))\n",
    "    return f\"\\n‚úÖ Best model: {best_model}\\nResponse:\\n{responses[best_model]}\"\n",
    "\n",
    "# === Step 4: Chain of Models ===\n",
    "\n",
    "def model_chain(article_text: str) -> str:\n",
    "    # Gemini extracts key facts\n",
    "    facts = call_gemini(\"Extract 3 key facts from this article:\\n\" + article_text)\n",
    "    print(\"\\nüîπ Gemini Facts:\\n\", facts)\n",
    "\n",
    "    # Claude rewrites them simply\n",
    "    simplified = call_claude(\"Rewrite the following simply:\\n\" + facts)\n",
    "    print(\"\\nüîπ Claude Simplified:\\n\", simplified)\n",
    "\n",
    "    # GPT-4 turns them into a tweet\n",
    "    tweet = call_gpt4(\"Create a tweet using these facts:\\n\" + simplified)\n",
    "    return f\"\\n‚úÖ Final Tweet:\\n{tweet}\"\n",
    "\n",
    "# === Step 5: Multi-Agent Workflow ===\n",
    "\n",
    "def multi_agent_blog_workflow(topic: str) -> str:\n",
    "    # Claude = planner\n",
    "    outline = call_claude(f\"Create a blog post outline on: {topic}\")\n",
    "    print(\"\\nüß† Claude (Planner) Outline:\\n\", outline)\n",
    "\n",
    "    # GPT-4 = writer\n",
    "    blog_post = call_gpt4(\"Write a blog post using this outline:\\n\" + outline)\n",
    "    print(\"\\n‚úçÔ∏è GPT-4 (Writer) Blog Post:\\n\", blog_post)\n",
    "\n",
    "    # Gemini = reviewer\n",
    "    review = call_gemini(\"Review this blog post and provide suggestions:\\n\" + blog_post)\n",
    "    return f\"\\n‚úÖ Gemini (Reviewer) Feedback:\\n{review}\"\n",
    "\n",
    "# === MAIN ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Explain quantum computing in simple terms.\"\n",
    "    article = \"Quantum computers use quantum bits or qubits which can be in superpositions, enabling them to solve certain problems exponentially faster than classical computers.\"\n",
    "\n",
    "    print(\"\\n=== STEP 1: Routing ===\")\n",
    "    print(route_by_task(\"summary\", prompt))\n",
    "\n",
    "    print(\"\\n=== STEP 2: Fallback ===\")\n",
    "    print(call_with_fallback(prompt))\n",
    "\n",
    "    print(\"\\n=== STEP 3: Voting / Evaluation ===\")\n",
    "    print(model_voting(prompt))\n",
    "\n",
    "    print(\"\\n=== STEP 4: Chain of Models ===\")\n",
    "    print(model_chain(article))\n",
    "\n",
    "    print(\"\\n=== STEP 5: Multi-Agent Workflow ===\")\n",
    "    print(multi_agent_blog_workflow(\"The Future of AI in Education\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to switch between cloud AI providers (OpenAI and Claude in this case) using a unified interface with minimal code changes.\n",
    "\n",
    "# üîÑ Full Python Code: Switch Between AI Providers (OpenAI & Claude)\n",
    "\n",
    "This code defines a common interface and lets you switch providers easily by changing a single line or config.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† `ai_provider.py`\n",
    "\n",
    "```python\n",
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "\n",
    "# === Common Interface ===\n",
    "class AIProvider:\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        raise NotImplementedError(\"Must implement 'generate' method\")\n",
    "\n",
    "# === OpenAI GPT-4 Adapter ===\n",
    "class OpenAIProvider(AIProvider):\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "# === Claude Adapter (Anthropic) ===\n",
    "class ClaudeProvider(AIProvider):\n",
    "    def __init__(self):\n",
    "        self.client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=300,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text.strip()\n",
    "\n",
    "# === Provider Selector (Factory) ===\n",
    "def get_provider(name: str) -> AIProvider:\n",
    "    if name == \"openai\":\n",
    "        return OpenAIProvider()\n",
    "    elif name == \"claude\":\n",
    "        return ClaudeProvider()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported AI provider: \" + name)\n",
    "\n",
    "\n",
    "# main code \n",
    "\n",
    "from ai_provider import get_provider\n",
    "import os\n",
    "\n",
    "# Choose provider via config/env\n",
    "provider_name = os.getenv(\"AI_PROVIDER\", \"openai\")  # change to \"claude\" to switch\n",
    "provider = get_provider(provider_name)\n",
    "\n",
    "# Prompt to run\n",
    "prompt = \"Explain the difference between AI and Machine Learning.\"\n",
    "\n",
    "# Get response\n",
    "response = provider.generate(prompt)\n",
    "\n",
    "# Output\n",
    "print(f\"\\nü§ñ Provider: {provider_name.capitalize()}\")\n",
    "print(\"üìù Response:\\n\", response)\n",
    "\n",
    "\n",
    "‚úÖ Benefits of This Design\n",
    "\n",
    "Feature\tAdvantage\n",
    "üîÑ Easily Switch\tChange one line or env var\n",
    "üß± Clean Code\tCommon interface, no duplicate logic\n",
    "‚ú® Extensible\tAdd more providers (e.g., Gemini, Cohere) easily\n",
    "üß™ Testable\tEach provider can be mocked/tested independently\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
