{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "# Define tool functions available to the agent\n",
    "def calculator_tool(expression):\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"Calculator result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def weather_api_tool(location):\n",
    "    fake_weather = {\"New York\": \"Sunny, 25°C\", \"Mumbai\": \"Cloudy, 30°C\"}\n",
    "    return f\"Weather in {location}: {fake_weather.get(location, 'No data')}\"\n",
    "\n",
    "# Registry for tool implementations\n",
    "tools = {\n",
    "    \"calculator\": calculator_tool,\n",
    "    \"weather\": weather_api_tool,\n",
    "}\n",
    "\n",
    "# Define the schema for function calling\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"A basic calculator\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\"type\": \"string\", \"description\": \"Expression to calculate, like '2 + 2'\"}\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"weather\",\n",
    "        \"description\": \"Get weather info for a city\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"City name, e.g. 'Mumbai'\"}\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def agent_respond_with_llm(user_query):\n",
    "    # Step 1: Ask the LLM to choose a tool via function calling\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # Use gpt-4-turbo or latest function-calling capable model\n",
    "        messages=[{\"role\": \"user\", \"content\": user_query}],\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",\n",
    "    )\n",
    "    choice = response.choices[0]\n",
    "    if choice.finish_reason == \"function_call\":\n",
    "        # LLM decided to use a tool; extract tool name and arguments\n",
    "        name = choice.message.function_call.name\n",
    "        args = eval(choice.message.function_call.arguments)\n",
    "        tool_response = tools[name](**args)\n",
    "        return tool_response\n",
    "    else:\n",
    "        # LLM chose to just respond naturally\n",
    "        return choice.message.content\n",
    "\n",
    "# Example test cases:\n",
    "print(agent_respond_with_llm(\"Calculate 7 * 8\"))\n",
    "print(agent_respond_with_llm(\"What's the weather in Mumbai?\"))\n",
    "print(agent_respond_with_llm(\"Tell me a joke.\"))  # Handled directly by the LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how an AI agent can use the “tool use” agentic pattern to interact with external tools (like APIs or web searches).\n",
    "\n",
    "## Connecting Agentic Patterns to Tool\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "# Define tool functions available to the agent\n",
    "def calculator_tool(expression):\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"Calculator result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def weather_api_tool(location):\n",
    "    fake_weather = {\"New York\": \"Sunny, 25°C\", \"Mumbai\": \"Cloudy, 30°C\"}\n",
    "    return f\"Weather in {location}: {fake_weather.get(location, 'No data')}\"\n",
    "\n",
    "# Registry for tool implementations\n",
    "tools = {\n",
    "    \"calculator\": calculator_tool,\n",
    "    \"weather\": weather_api_tool,\n",
    "}\n",
    "\n",
    "# Define the schema for function calling\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"A basic calculator\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\"type\": \"string\", \"description\": \"Expression to calculate, like '2 + 2'\"}\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"weather\",\n",
    "        \"description\": \"Get weather info for a city\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"City name, e.g. 'Mumbai'\"}\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def agent_respond_with_llm(user_query):\n",
    "    # Step 1: Ask the LLM to choose a tool via function calling\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # Use gpt-4-turbo or latest function-calling capable model\n",
    "        messages=[{\"role\": \"user\", \"content\": user_query}],\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",\n",
    "    )\n",
    "    choice = response.choices[0]\n",
    "    if choice.finish_reason == \"function_call\":\n",
    "        # LLM decided to use a tool; extract tool name and arguments\n",
    "        name = choice.message.function_call.name\n",
    "        args = eval(choice.message.function_call.arguments)\n",
    "        tool_response = tools[name](**args)\n",
    "        return tool_response\n",
    "    else:\n",
    "        # LLM chose to just respond naturally\n",
    "        return choice.message.content\n",
    "\n",
    "# Example test cases:\n",
    "print(agent_respond_with_llm(\"Calculate 7 * 8\"))\n",
    "print(agent_respond_with_llm(\"What's the weather in Mumbai?\"))\n",
    "print(agent_respond_with_llm(\"Tell me a joke.\"))  # Handled directly by the LLM\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png\" class=\"logo\" width=\"120\"/>\n",
    "\n",
    "# What are the essential building blocks for creating effective AI agents? write point wise in tabular format . use markdown\n",
    "\n",
    "| Building Block | Description |\n",
    "| :-- | :-- |\n",
    "| Goal Definition | Clear specification of the agent’s primary objective or task. |\n",
    "| Perception/Input | Mechanisms for receiving and interpreting external data (e.g., user queries, sensor data, APIs). |\n",
    "| Reasoning \\& Planning | Logic or models (often LLMs) for understanding context, decomposing problems, and selecting steps |\n",
    "| Tool Use/Action Modules | Libraries or APIs that enable the agent to interact with external systems or perform computations. |\n",
    "| Memory/State Management | Components for storing context, conversation history, or relevant world/model knowledge. |\n",
    "| Decision-Making Policy | Framework or algorithms that help the agent choose when/which action or tool to invoke. |\n",
    "| Learning \\& Adaptation | Ability to improve through feedback, experience, or user corrections (e.g., RL, fine-tuning). |\n",
    "| Interface/Communication | Systems for engaging with users or other agents (e.g., chat UIs, voice, API endpoints). |\n",
    "| Safety \\& Alignment | Constraints and safeguards to ensure agent acts within ethical, legal, and intended boundaries. |\n",
    "| Modularity \\& Extensibility | Design that allows easy integration of new tools, skills, and capabilities. |\n",
    "\n",
    "\n",
    "# write a sample python code using groq api for each Building Block\n",
    "\n",
    "1. Goal Definition\n",
    "# Agent's main objective, defined for prompt clarity and reasoning\n",
    "AGENT_GOAL = \"Summarize scientific articles so non-experts can understand them.\"\n",
    "\n",
    "2. Perception/Input\n",
    "# Example: Receive input from user or an external API\n",
    "user_query = input(\"Ask the agent: \")\n",
    "\n",
    "3. Reasoning & Planning (LLM via Groq)\n",
    "\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "# Standard LLM call for reasoning/planning, providing system prompt for goal/context\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are an agent. Goal: {AGENT_GOAL}\"},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\"\n",
    ")\n",
    "print(\"Agent:\", response.choices[0].message.content)\n",
    "\n",
    "\n",
    "4. Tool Use / Action Modules\n",
    "\n",
    "# Simple tool functions, agent decides (can also be triggered by the LLM's suggestion)\n",
    "def search_tool(query):\n",
    "    # Placeholder: In production, call a search API\n",
    "    return f\"Search results for '{query}': [stub]\"\n",
    "\n",
    "def math_tool(expression):\n",
    "    try:\n",
    "        return f\"Result: {eval(expression)}\"\n",
    "    except:\n",
    "        return \"Invalid math expression.\"\n",
    "\n",
    "# Example usage:\n",
    "# result = search_tool(\"Groq API advantages\")\n",
    "# print(result)\n",
    "\n",
    "5. Memory / State Management\n",
    "\n",
    "# Maintain chat/conversation history for continuity and context\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": f\"Agent goal: {AGENT_GOAL}\"}\n",
    "]\n",
    "conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "\n",
    "# For context persistence, pass conversation_history to future LLM calls\n",
    "\n",
    "6. Decision-Making Policy\n",
    "# Decide next action/tool based on analysis (simple rule-based, LLM-powered, or both)\n",
    "def choose_action(query):\n",
    "    if \"search\" in query.lower():\n",
    "        return search_tool(query)\n",
    "    elif any(char.isdigit() for char in query) and any(op in query for op in \"+-*/\"):\n",
    "        return math_tool(query)\n",
    "    else:\n",
    "        # Default to LLM\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "7. Learning & Adaptation\n",
    "\n",
    "# Collect user feedback for incremental improvement/future tuning\n",
    "feedback = input(\"Was this answer helpful? (y/n): \")\n",
    "log_entry = {\n",
    "    \"query\": user_query,\n",
    "    \"response\": response.choices[0].message.content,\n",
    "    \"feedback\": feedback\n",
    "}\n",
    "# Store log_entry for analysis or model fine-tuning downstream\n",
    "\n",
    "8. Safety & Alignment\n",
    "\n",
    "# Filter/block unsafe requests before dispatching to the LLM\n",
    "unsafe_keywords = [\"hack\", \"illegal\", \"scam\"]\n",
    "if any(word in user_query.lower() for word in unsafe_keywords):\n",
    "    print(\"Request blocked for safety reasons.\")\n",
    "else:\n",
    "    # Proceed to agent reasoning, tool use, etc.\n",
    "    pass\n",
    "\n",
    "9. Modularity & Extensibility\n",
    "\n",
    "# Easily add new tools or swap components with a Python dictionary registry\n",
    "TOOLS = {\n",
    "    \"search\": search_tool,\n",
    "    \"math\": math_tool,\n",
    "    # Add more as needed...\n",
    "}\n",
    "\n",
    "# Example: Dynamically call a tool\n",
    "tool_to_use = \"search\"\n",
    "if tool_to_use in TOOLS:\n",
    "    output = TOOLS[tool_to_use](\"Groq API\")\n",
    "    print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
